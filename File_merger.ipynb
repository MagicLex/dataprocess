{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d59bd4-a953-4575-bfa7-b63aecc428c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97227a60-06a8-4b86-a246-2ad68c15862c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory path for the CSV files\n",
    "directory_path = \"../Zoom_Downloads\"\n",
    "\n",
    "# Initialize a dictionary to keep track of schema matches and a counter for rows\n",
    "schemas = {}\n",
    "total_rows_before_merge = 0\n",
    "\n",
    "# List to keep track of dataframes for merging\n",
    "dataframes_to_merge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00aaa87c-9884-4966-be4c-0488cce319ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Companies NYC BOS PHL (2).csv\n",
      "Banking EU-7UywK6-enhance.csv\n",
      "Removed duplicates. Rows after deduplication: 187650.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('../Zoom_Downloads/merged_files.csv')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get the schema of a CSV file\n",
    "def get_schema(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=1, low_memory=False)  # read only the first row to get the schema\n",
    "        return df.columns.tolist()  # return the column headers as a list\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to count the rows of a CSV file\n",
    "def count_rows(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path, low_memory=False).shape[0]  # return the number of rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Process the files\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.csv'):  # Check only csv files\n",
    "        file_path = Path(directory_path) / file_name\n",
    "        schema = get_schema(file_path)\n",
    "        if schema is not None:\n",
    "            schema_key = tuple(schema)  # convert the list to a tuple to use as a dict key\n",
    "            if schema_key in schemas:\n",
    "                # If schema matches, append the dataframe to the list and update row count\n",
    "                dataframes_to_merge.append(pd.read_csv(file_path, low_memory=False))\n",
    "                total_rows_before_merge += count_rows(file_path)\n",
    "            else:\n",
    "                # If new schema, print the file name and add to the dict\n",
    "                print(file_name)\n",
    "                schemas[schema_key] = file_name\n",
    "\n",
    "# Merge all dataframes with the same schema\n",
    "merged_df = pd.concat(dataframes_to_merge, ignore_index=True) if dataframes_to_merge else None\n",
    "\n",
    "# Function to compare row count and remove duplicates\n",
    "def process_merged_dataframe(merged_df, total_rows_before_merge):\n",
    "    if merged_df is not None:\n",
    "        # Check for discrepancies in row count\n",
    "        total_rows_after_merge = merged_df.shape[0]\n",
    "        if total_rows_before_merge != total_rows_after_merge:\n",
    "            print(f\"Row count mismatch: {total_rows_before_merge} before, {total_rows_after_merge} after merge.\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        merged_df.drop_duplicates(inplace=True)\n",
    "        print(f\"Removed duplicates. Rows after deduplication: {merged_df.shape[0]}.\")\n",
    "\n",
    "# Check if we have a merged dataframe, process it, and output a file if we do\n",
    "output_file_path = None\n",
    "if merged_df is not None:\n",
    "    process_merged_dataframe(merged_df, total_rows_before_merge)\n",
    "    output_file_path = Path(directory_path) / 'merged_files.csv'\n",
    "    merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# The output file path will point to the final merged CSV\n",
    "output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31bf43f1-21bd-4b70-b411-986afd4d8a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reference file\n",
    "file_path_df1 = \"../Zoom_Downloads/249_100423_PERSON (1).csv\"\n",
    "\n",
    "#To Change \n",
    "file_path_df2 = \"../Zoom_Downloads/Target Companies NYC BOS PHL (2).csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c452ffc3-4726-4134-8c37-ee1beb05911a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns unique to DF1 (reference) (0):\n",
      "set()\n",
      "\n",
      "Columns unique to DF2 (to change) (0):\n",
      "set()\n",
      "\n",
      "Common columns (71):\n",
      "{'Facebook Company Profile URL', 'Person State', 'Company Country', 'Revenue (in 000s USD)', 'LinkedIn Contact Profile URL', 'Twitter Company Profile URL', 'Full Address', 'Last Name', 'Ownership Type', 'Direct Phone Number', 'SIC Codes', 'Secondary Industry Hierarchical Category', 'Founded Year', 'Number of Locations', 'Recent Funding Round', 'Email Domain', 'Contact Accuracy Grade', 'Middle Name', 'Recent Funding Date', 'Industry Hierarchical Category', 'ZoomInfo Contact ID', 'Fax', 'All Sub-Industries', 'Alexa Rank', 'ZoomInfo Contact Profile URL', 'Revenue Range (in USD)', 'All Industries', 'Notice Provided Date', 'Primary Sub-Industry', 'Company HQ Phone', 'Suffix', 'SIC Code 2', 'LinkedIn Company Profile URL', 'Company Street Address', 'Company State', 'Management Level', 'NAICS Code 2', 'Certified Active Company', 'Certification Date', 'Recent Investors', 'Company City', 'Company Zip Code', 'Person Street', 'Employees', 'Business Model', 'Contact Accuracy Score', 'NAICS Codes', 'Employee Range', 'First Name', 'Ticker', 'ZoomInfo Company ID', 'Primary Industry', 'ZoomInfo Company Profile URL', 'Recent Funding Amount (in 000s USD)', 'Person Zip Code', 'Query Name', 'Website', 'NAICS Code 1', 'Country', 'Salutation', 'Mobile phone', 'SIC Code 1', 'Person City', 'Company Name', 'Department', 'All Investors', 'Job Function', 'Email Address', 'Job Title', 'Company Division Name', 'Total Funding Amount (in 000s USD)'}\n"
     ]
    }
   ],
   "source": [
    "# Load the first file and get the info\n",
    "df1 = pd.read_csv(file_path_df1)\n",
    "df1_info = df1.columns.tolist()\n",
    "\n",
    "# Load the second file and get the info\n",
    "df2 = pd.read_csv(file_path_df2, low_memory=False)\n",
    "df2_info = df2.columns.tolist()\n",
    "\n",
    "# Compare and highlight differences\n",
    "unique_to_df1 = set(df1_info) - set(df2_info)\n",
    "unique_to_df2 = set(df2_info) - set(df1_info)\n",
    "common_columns = set(df1_info) & set(df2_info)\n",
    "\n",
    "# Count the unique and common columns\n",
    "unique_to_df1_count = len(unique_to_df1)\n",
    "unique_to_df2_count = len(unique_to_df2)\n",
    "common_columns_count = len(common_columns)\n",
    "\n",
    "print(f\"Columns unique to DF1 (reference) ({unique_to_df1_count}):\")\n",
    "print(unique_to_df1)\n",
    "print(f\"\\nColumns unique to DF2 (to change) ({unique_to_df2_count}):\")\n",
    "print(unique_to_df2)\n",
    "print(f\"\\nCommon columns ({common_columns_count}):\")\n",
    "print(common_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270be5f1-79e6-42e1-bd65-04f7a326dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['']\n",
    "\n",
    "df2 = pd.read_csv(file_path_df2, low_memory=False)\n",
    "df2 = df2.drop(columns_to_drop, axis=1)\n",
    "df2.to_csv(file_path_df2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76a9a6d3-6583-4183-b780-a9514b85d69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_add = {'Query Name': 'none'} \n",
    "df2 = pd.read_csv(file_path_df2, low_memory=False)\n",
    "\n",
    "# Add the new column(s)\n",
    "for column_name, default_value in columns_to_add.items():\n",
    "    df2[column_name] = default_value\n",
    "\n",
    "df2.to_csv(file_path_df2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7220bb-3be4-496b-802a-72e77d8ea15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Re-Ordering of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af313289-e2d6-4753-9973-5536163d3e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the first file and get its column order\n",
    "df1 = pd.read_csv(file_path_df1)\n",
    "df1_info = df1.columns.tolist()\n",
    "\n",
    "# Load the second file\n",
    "df2 = pd.read_csv(file_path_df2, low_memory=False)\n",
    "\n",
    "# Reorder df2's columns to match df1's order, dropping any that don't exist in df1 and adding missing ones as NaN\n",
    "df2_reordered = df2.reindex(columns=df1_info).assign(**{col: pd.NA for col in df1_info if col not in df2.columns})\n",
    "\n",
    "# Save the reordered df2 back to CSV\n",
    "df2_reordered.to_csv(file_path_df2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "89f9b475-9a2a-4f23-9953-ef00481775a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Matching type with DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20767914-4a72-4c79-b959-929e6d1b574f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the first file\n",
    "df1 = pd.read_csv(file_path_df1)\n",
    "\n",
    "# Determine the data types in df1\n",
    "dtype_dict = df1.dtypes.to_dict()\n",
    "\n",
    "# Load the second file\n",
    "df2 = pd.read_csv(file_path_df2, low_memory=False)\n",
    "\n",
    "# Replace NaN values with an appropriate value based on the column data type\n",
    "for col in df2.columns:\n",
    "    if col in dtype_dict:\n",
    "        if dtype_dict[col] == 'float64' or dtype_dict[col] == 'int64':\n",
    "            replacement_value = 0  # Or df2[col].mean() for mean replacement\n",
    "        elif dtype_dict[col] == 'object':\n",
    "            replacement_value = 'Unknown'\n",
    "        df2[col].fillna(replacement_value, inplace=True)\n",
    "\n",
    "# Now attempt to convert data types of df2 columns to match df1 where possible\n",
    "for col in df2.columns:\n",
    "    if col in dtype_dict:\n",
    "        try:\n",
    "            df2[col] = df2[col].astype(dtype_dict[col])\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not convert column {col} to {dtype_dict[col]} due to: {e}\")\n",
    "\n",
    "# Save the NaN-replaced and converted df2 back to CSV\n",
    "df2.to_csv(file_path_df2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13de3d61-0509-4c81-892d-08e42b5bd3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count per column:\n",
      " ZoomInfo Contact ID    0\n",
      "Last Name              0\n",
      "First Name             0\n",
      "Middle Name            0\n",
      "Salutation             0\n",
      "                      ..\n",
      "Company Zip Code       0\n",
      "Company Country        0\n",
      "Full Address           0\n",
      "Number of Locations    0\n",
      "Query Name             0\n",
      "Length: 71, dtype: int64\n",
      "\n",
      "Total NaN count in df2: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df2 is already loaded\n",
    "# file_path_df2 = \"../Zoom_Downloads/Gothenburg list-vR47vY-enhance.csv\"\n",
    "# df2 = pd.read_csv(file_path_df2, low_memory=False)\n",
    "\n",
    "# Count NaN values for each column\n",
    "nan_count_per_column = df2.isna().sum()\n",
    "\n",
    "# Count total NaN values in the DataFrame\n",
    "total_nan_count = df2.isna().sum().sum()\n",
    "\n",
    "print(\"NaN count per column:\\n\", nan_count_per_column)\n",
    "print(\"\\nTotal NaN count in df2:\", total_nan_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce8785bb-28df-4608-adcf-e714cf471b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaN values per column:\n",
      " ZoomInfo Contact ID    0.0\n",
      "Last Name              0.0\n",
      "First Name             0.0\n",
      "Middle Name            0.0\n",
      "Salutation             0.0\n",
      "                      ... \n",
      "Company Zip Code       0.0\n",
      "Company Country        0.0\n",
      "Full Address           0.0\n",
      "Number of Locations    0.0\n",
      "Query Name             0.0\n",
      "Length: 71, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nan_count_per_column = df2.isna().sum()\n",
    "total_rows = len(df2)\n",
    "\n",
    "# Calculate the percentage of NaN values per column\n",
    "nan_percentage_per_column = (nan_count_per_column / total_rows) * 100\n",
    "\n",
    "# Display the percentage of NaN values per column\n",
    "print(\"Percentage of NaN values per column:\\n\", nan_percentage_per_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e917c-7eee-47c9-b0b3-a4e96fb14996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519e878-8a65-4bed-a41b-a16ca32b768f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
